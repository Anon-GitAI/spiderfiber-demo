{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43be511a-538f-424d-a070-6dd506ba0bd0",
   "metadata": {},
   "source": [
    "# Sample inference code for SpiderGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a90717fd-dd00-4666-a036-1439968499f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import Trainer, TrainingArguments,DataCollatorForLanguageModeling\n",
    "import re\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "trained_model_name='anon-genAI/spiderfiber-anon'\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_name = trained_model_name\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "195c18e2-b540-4bc2-8eb5-986c19f55b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EstimateProperties<AAAGGAGQGGYGGQGAGQGAAAAAAGGAGQGGYGGQGAGQGAGAAAAAAGGAGQGGYGGLGSGQGGYGGQG AGAAAAAAAAGGAGQGGYGGLGSGQGGYGGQGAGAAAAAAGGAGQGGYGGLGGQGAGQGSGAAAAAAGGAGQGGYGGQ  GAGQGAGAAAAAAGGAGQGGYGGLGGQGAGQGAAAAAAGGAGQ GGYGGQGAGQGAGAAAAAAGGAGQGGYGGLGSGQGGYGGQ  GAGAAAAAAGGAGQGGYGGLGGQGAGAAAAAAGGAGQGG YGGQGAGQGAAAAAAGGAGQGGYGGQGAGQGGYGGQGA GAAAAAAGGAGQGGYGGLGGQGAGQGAGAAA  AAAGGAGQGGYGGQGAGQGAGAAAAAAGGAGQGGYGGLGGQGA GAAAAAAGGAGQGGYGGQGAGQGGYGGQGSGAAAAAAAA   GGAGQGGYGGLGSQGAGQGAGAAAAAAGGAGQGGYGG QGAGQGAGAAAAAAGGAGQGGYGGQGAGQGAGAAAAAAGGAGQGG   YGGQGAGQGAGAAAAAAGGAGQGGYGGLGSGQGGY GGQGAGAAAAAAGGAGQGGYGGQGAGAAAASAAASRLSSPEASSGLS   GCDVLVQALLEVVSALIHILGSSSIGPVNYGSASQSTQIVGQSVYQALG> [0.23,0.22,0.36,0.35,0.34,0.38,0.43,0.09]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"EstimateProperties<AAAGGAGQGGYGGQGAGQGAAAAAAGGAGQGGYGGQGAGQGAGAAAAAAGGAGQGGYGGLGSGQGGYGGQG\\\n",
    " AGAAAAAAAAGGAGQGGYGGLGSGQGGYGGQGAGAAAAAAGGAGQGGYGGLGGQGAGQGSGAAAAAAGGAGQGGYGGQ \\\n",
    " GAGQGAGAAAAAAGGAGQGGYGGLGGQGAGQGAAAAAAGGAGQ GGYGGQGAGQGAGAAAAAAGGAGQGGYGGLGSGQGGYGGQ \\\n",
    " GAGAAAAAAGGAGQGGYGGLGGQGAGAAAAAAGGAGQGG YGGQGAGQGAAAAAAGGAGQGGYGGQGAGQGGYGGQGA GAAAAAAGGAGQGGYGGLGGQGAGQGAGAAA\\\n",
    "  AAAGGAGQGGYGGQGAGQGAGAAAAAAGGAGQGGYGGLGGQGA GAAAAAAGGAGQGGYGGQGAGQGGYGGQGSGAAAAAAAA \\\n",
    "  GGAGQGGYGGLGSQGAGQGAGAAAAAAGGAGQGGYGG QGAGQGAGAAAAAAGGAGQGGYGGQGAGQGAGAAAAAAGGAGQGG \\\n",
    "  YGGQGAGQGAGAAAAAAGGAGQGGYGGLGSGQGGY GGQGAGAAAAAAGGAGQGGYGGQGAGAAAASAAASRLSSPEASSGLS \\\n",
    "  GCDVLVQALLEVVSALIHILGSSSIGPVNYGSASQSTQIVGQSVYQALG>\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt, add_special_tokens = False)).unsqueeze(0).to(device)\n",
    "# print(generated.shape, generated)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                inputs=generated, \n",
    "                                eos_token_id =tokenizer.eos_token_id,\n",
    "                                do_sample=False,   \n",
    "                                top_k=500, \n",
    "                                max_length = 300,\n",
    "                                top_p=0.9, \n",
    "                                num_return_sequences=1,\n",
    "                                temperature=1,\n",
    "                                max_new_tokens=400\n",
    "                                ).to(device)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(tokenizer.decode(sample_output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b6c1bc7-3a01-4959-a786-2078d06bc16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 57]) tensor([[ 43, 299,  73,  86,  69,  88,  73, 303,  32,  20,  18,  21,  27,  27,\n",
      "          16,  20,  18,  22,  22,  22,  16,  20,  18,  20,  28,  22,  16,  20,\n",
      "          18,  20,  26,  25,  16,  20,  18,  22,  22,  25,  16,  20,  18,  22,\n",
      "          24,  21,  16,  20,  18,  22,  26,  26,  16,  20,  18,  25,  21,  25,\n",
      "          34]], device='cuda:0')\n",
      "GenerateSequence<0.177,0.222,0.082,0.065,0.225,0.241,0.266,0.515> [NDISSASSASAVSDGQGGYGQEQSPRAGTGSAGQDQVGYGGQGGVSASASAGVAGGAGTATEGGYGGPGAGSGGAGAPGGYGPAGPGTGSLNNQGGYGPGAGAGAAAVSSASVGAGSQGYGPSGYTSGTGASGPGGASGAAAAAAAATGGYGRAGPGAAAAAAAAGQGGYGQGGQGTGAAAAAAAGGSGGQGQGSGAAAAAAAASGQGGQGGYGQGGQSGQGGQGGYGQGGQGYGQQGAGAGAAAAAAAAAGQGGQGGYGQGGQGGYGQGSSGAAAAAAAAAAGGSGGQGGQGGYGQGGQGGYGQGAAAAAAAAAGGTGGQGGYGQGAGSGQGGQGGYGQGGQGGYGQGAAAAAAASGLSGQGRGAGQGGQGGYGQGGQGGYGQGAAAAAAAGGSGQGGYGQGPQIGQGSGAAAAAAAAAGRGGYGQGAGPGGAGQGGQGGYGQGGQSGQGGQGGYGQGGQGAGAAAAAAAAGGAGGAGRGGYGQGAGPGGAGAAAAAAAAAAGGQGGQGGYGQGGYGQGGIGGYSQRTAGAGSAAATGGQGPGGYGQGSGPRSASVAAAGGGQGGQGGYGQG]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"GenerateSequence<0.177,0.222,0.082,0.065,0.225,0.241,0.266,0.515>\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt, add_special_tokens = False)).unsqueeze(0).to(device)\n",
    "print(generated.shape, generated)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                inputs=generated, \n",
    "                                eos_token_id =tokenizer.eos_token_id,\n",
    "                                do_sample=True,   \n",
    "                                top_k=50, \n",
    "                                max_length = 300,\n",
    "                                top_p=0.9, \n",
    "                                num_return_sequences=1,\n",
    "                                temperature=1,\n",
    "                                max_new_tokens=400\n",
    "                                ).to(device)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(tokenizer.decode(sample_output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "effc701c-8d4e-43cf-ab30-9d59f0a2a37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Both `max_new_tokens` (=300) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3]) tensor([[ 303,   32, 1988]], device='cuda:0')\n",
      "0: Sequence<AAAGPGGYGPSQRGPSGPGSAAAAAAGAGPGGYGPGQQGPSGPGSAAAAATAAGGPGGYGPGQQGPGGYGPSGPSGPGGAGPYAAAAAAAAGGPGGYGPGAQGPSGPSNGPGQQGPGGYGPSGPGASAAAAAASGPGGRGPSGPSGPGGAAAAAAAAAGGPGGYGPSQQGPGGYGPSGPGGPGGAAAAAGGPGGYGPGSQGPGGPGASAAAAAASGPGGSGPGGYGPSQQGPGQQGPGGYGPSGPGGASAAAAAAAAAASGPGGNNGYGPGGPGQQGPGGYGPGGSGPGGASAAAAAAGGAGGPGGYGPGGYGPGSQGPSGPGGYGPSSQGPGAAGGAGGPGSQGPYGPGSQGGYGPGGSGPAAAASSSAASGPGGYGPGSQGPSVNAAAAAAGGSGPGGYGPGGYGPGPSGPGGAGAAAAAAAASGPGGYGPGSQGPSGPAGYGPSGLSGPGGAAAAAASGPGGYGPGSQGPSGPRGYSQGLGPGGAASAAAAAAGGPGGYGPANQGPSGSSSGPGGASAAAAAAAGGLGGQGPSGPGSQGPSGPGGYGPGSQGPGGYGPGSQGPGGPGASAAAAAASGPGGYGPGSQGPSGPGSQGPSGPGGYGPGSQGPGGYGPGSQGPSGYGPSGPGGASAAAAAASASGPGGPGSQGPSGPSGPGGYGPGSQGPSGPGGYGPGASAAAAAAASGPGGYGPGSQGPSGPGSQGPSGPGGPGASAAAAAASGPGGYGPGSQGPSGPSGPGGYGPGSQGPSGPGGYGP>\n",
      "\n",
      "\n",
      "1: Sequence<AAAGAAAAAAAAGGQGGQGGYGSQGAGQGGYGAGQGGAGAAGAAAAAAAAGGAGGSGQGGLLAGGAGQGYGAGLGGQGGDGQGGAGAAASAAAAGGAGGQGGYGGLGSQGAGQGGYGSGGAGAAAAAAAAGGQGGQGGYGSQGAGQGGYGAGQGGAGAAAAAAAAGGAGGSGRS>\n",
      "\n",
      "\n",
      "2: Sequence<AAAGAGGAGGLGGGAGAGAGVGVGAGAGAGAGAGAGAGAGGAGGYGGGYGAGSGAGAGSSASAGGAGAGGSRGYGGAGGYGPGAGTGAASRAGGAGGYGGDYGAGAGAGAGAGAAAGGAGGYGAADGGRRSAAAAAAAAAAAAGSSSRTGGYGAGAGAGAGGAGGYGGYGAGSGAGAGAGAGAGAGGYGGGYGAAAAAAAAAAEAGAGGAGGYGGGTGAGAGAGAGAGAGGYGAGSGAGGYGAGIAASGAAAAAGAGGAGGYGGGAGAGAGAAAGAGAGAGAGGAGGYGAGRGGAGAGAGAAAGAGAGAGAGGAGGYGAGRGVGYGAGAGAGAGAGAAAGGAGGYGGGAGAGAGAAAADGGAGGYNPQGALGYGTGQGYGAGSGAGAGAKTAGGYGSGSGAGAGSGAGAGAAAAAGAGGAGGYGGGAGAGAGAAAAAGAGGNYGLYSSGYGGGTEAGAGAGAAAAAGAGGAGGYGGGAGAGAGAGSATIGYGYGYGNGAGAGAGAAAAAGAGAGAGAGAAAGAGAGGAGGYGGGYGGGTEGAGAGAGAGAGAAAAAGSGIAGGYGGGYGAGAGAGAAAGSYGGGFGGFGVGGAVASSGSYGAGATQSYGTSGSRLSSAEASSRIASAASSLNAGGIFNRIALSKLADVISLSGAFGGGVALVGNVGYRLGLAGAGAGAAAGAGYVSGAGGAGGLGGYGLGSGVGVGSTVSTGSSSLSKYLSNQLSSNFAGKGYGDQGIGGAGLARSAGAGAGAGMDFGAGPGAGLGAGVGAAAAAGAGGGQGGYGSGLGYGGGAGAGGGAGAAGGVGGYGGQGAGAGAAAGAAAAGAGGAAGYGYGGGAPLGGYGGGAGAGAAAAAAAGAGAGGYGGGYGAGQGYGAGAGAGAAAAAGTGAAGGYGQGGAGYGAGTGVAGAAAGAGAGAGAGAGVGGLGGYGQGYGAGAGGSAAAAAAGAGAGGFGRGAGAGAGAGAGSGASAGGVAESFPLNTVNRMGPGAAGAGYATYRVSSNSFGSGADGAYDQTITVSIPAGDTVSYFDAAQSGYGQDTYGAAAGSLCLSFGSHLYAATNLRINGGSYADPGYGAGGARAGAGAAGAGDGFAVGYGAGAAAGAGSGAGRAGGYGRGAGAGAAAVAGAGALADAYGGGYGAGAGAGFGVAGAAAGAAASGRGAGQRGQGAGAGAAAGAGAGAGAGSGGAGGYGGDYGAAAAAAAAAAAAAAGGAAGAGAGSGAGTGAGAAAAAGAGGAGGYGGGAG>\n",
      "\n",
      "\n",
      "3: Sequence<AAAGPGGYGPSQRGPSGPGSAAAAAAGAGPGGYGPGQQGPSGPGSAAAAATGAGPGGYGPGQQGPGGYGPSGPSGPGGASAAAAAAAGPGGYGPGQQGPGQQGPGQQGPAGYGPSGLSGPGGAAAAAAAAAGPGGYGPGQQGPSGPGGAAAAAAAAGPGGYGPSQRGPSGPGSAAAAAAGAGPGGYGPGQQGPSGPGSAAAAATGAGPGGYGPGQQGPGGYGPSGPSGPGGASAAAAAAAGPGGYGPGQQGPGQQGPGQQGPAGYGPSGLSGPGGAAAAAAAGPGGYGPGQQGPSGPGGAAAAAAAAGPGGYGPSQRGPSGPGSAAAAAAGAGPGGYGPGQQGPSGPGSAAAAAAAGPGGYGPSQQGPARYGPSGPGSAAAAAAGAGTAGYGPGPQASAAASRLASPDSGARVASAVSNLVSSGPTSSAALSSSMATSVGQDQSSWAYN>\n",
      "\n",
      "\n",
      "4: Sequence<AAAGAGGQGGYGGLGSQGAGQGGYGAGQGGAGAAAAAAAAGGAGGAGQGGLGAGGAGQGYGAGQGYGAGQGGAGQGGAAAAAAAAGGLGGYGGLGSQGAGQGGYGAGQGGAGAAAAAAAAGGAGGAGQGGLGAGGAGQGYGAGQGYGAGLGGQGGAGQGGAAAAAAAAAAGGQGGQGGYGGLGSQGAGQGGLGTDSGAFYISNGLGGQGGMSVG>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"GenerateSilkContent<0.177,0.222,0.082,0.065,0.225,0.241,0.266,0.515>\"\n",
    "prompt = \"Sequence<AAAG\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt, add_special_tokens = False)).unsqueeze(0).to(device)\n",
    "print(generated.shape, generated)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                inputs=generated, \n",
    "                                eos_token_id =tokenizer.eos_token_id,\n",
    "                                do_sample=True,  \n",
    "                                max_length = 200,\n",
    "                                num_return_sequences=2,\n",
    "                                max_new_tokens=300\n",
    "                                ).to(device)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    out_string=tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    print(\"{}: {}\\n\\n\".format(i, out_string))\n",
    "    num_tokens = len(out_string)\n",
    "    if num_tokens < 50:\n",
    "        print(f\"⚠️ Output only contains {num_tokens} tokens — likely incomplete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a825cd-8f3d-46c6-b248-01e638dc800b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
